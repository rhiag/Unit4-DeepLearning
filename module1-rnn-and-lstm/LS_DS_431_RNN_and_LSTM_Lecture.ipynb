{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et2y0gP7IM19"
   },
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "![](https://wiki.tum.de/download/attachments/22578349/GATES.gif?version=1&modificationDate=1486083227237&api=v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BOMScPtIM1-"
   },
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IizNKWLomoA"
   },
   "source": [
    "-----\n",
    "# Overview\n",
    "\n",
    "### Let's start with sequences \n",
    "\n",
    "A sequence is just any collection of numbers - order counts and repetition is allowed. \n",
    "\n",
    "Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list and is different from `[1, 2, -1, 2]`. \n",
    "\n",
    "What you might not be as familiar with are recusive numbers. For that, let's talk about a specific example, namely the **Fibonacci Sequence**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX_WLYHrIM1_"
   },
   "source": [
    "\n",
    "\n",
    "Before we dive into the inner workings of an LSTM model, let's try to understand and appreciate the recusive relationships of numbers in both pure mathematics and in the physical reality in which we find ourselves embedded. \n",
    "\n",
    "\n",
    "As usually we take attempt to understand a concept from at least 3 different perspectives:\n",
    "- Algebraic\n",
    "- Geometric\n",
    "- Coding an example\n",
    "\n",
    "\n",
    "A [**recurrence relation**](https://en.wikipedia.org/wiki/Recurrence_relation) in math is an equation that uses recursion to define a sequence of numbers - a famous example is the Fibonacci numbers.\n",
    "\n",
    "Here is the algorithm for generating the numbers in the Fibonacci sequence: \n",
    "\n",
    "$$F_n = F_{n-1} + F_{n-2}$$\n",
    "\n",
    "You need a base case $F_0=1, F_1=1$ (i.e. a starting point) to get the sequence started and then from then on our the sequence is self-generating. \n",
    "\n",
    "So this means that we can start generating our sequence: \n",
    "\n",
    "$$F_0=1,~~  F_1=1 $$\n",
    "\n",
    "$$F_2 = F_{1} + F_{0} ~=~ 1 + 1 ~=~ 2$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$F_3 = F_{2} + F_{1} ~=~ 2 + 1 ~=~ 3$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$F_4 = F_{3} + F_{2} ~=~ 3 + 2 ~=~ 5$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$F_5 = F_{4} + F_{3} ~=~ 5 + 3 ~=~ 8$$\n",
    "\n",
    "I hope you get the idea. \n",
    "\n",
    "Before we we code up this sequence, let's appreciate how important and ubiquitous it is in nature. \n",
    "\n",
    "![](http://www.davidbeahm.com/wp-content/uploads/2011/11/fibonacci-1024x637.jpg)\n",
    "\n",
    "\n",
    "![](https://i.pinimg.com/originals/32/d7/47/32d747bea24f4756dc4c5ffe61b36efd.jpg)\n",
    "\n",
    "![](https://i.pinimg.com/originals/f2/cb/34/f2cb3452dd774bab87bbee2b8a77d4bb.png)\n",
    "\n",
    "\n",
    "![](https://f4.bcbits.com/img/a3628582449_10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take Away:** \n",
    "- Recursive sequences are located everywhere in life - but we need to know what we're looking for and where to look for it. \n",
    "- Simply try to develop an appreciation for the connection between mathematics and all of physical reality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code up the Fibonacci Sequence\n",
    "Again, here is the algorithm for the Fibonacci numbers.  \n",
    "\n",
    "\n",
    "$$F_n = F_{n-1} + F_{n-2}$$\n",
    "\n",
    "\n",
    "You need a base case to get your sequence started. This time let  $F_0=0 ~\\text{and}~ F_1=1$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibo(n):\n",
    "    \"\"\"\n",
    "    Calculate and return the next number in the Fibonacci sequence\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    n: int or float\n",
    "        The nth number in the sequence (think of it as an index for a list)\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    F_n: the next number in the sequence generated from the previous two numbers in the sequence \n",
    "    \"\"\"\n",
    "    \n",
    "    if n <= 1:\n",
    "        # if n = 0, then return 0 \n",
    "        return n\n",
    "    else:\n",
    "        # this is the recursive part \n",
    "        # notice how the function is a function of itself!\n",
    "        #  F_n =       F_n-1 + F_n-2\n",
    "        return(fibo(n-1) + fibo(n-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5757486eecafe9c2c1af5b428e482b3",
     "grade": false,
     "grade_id": "cell-b31ecb0aaf3ace76",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# generate a Fibonacci Sequence\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take Away:** \n",
    "\n",
    "Recursive algorithms have as input their previous output. In order words, the output at time step `t - 1`, becomes in the input in the following time step `t`. This is the key idea of that you should observe. Because it is this recursive behavior that is new to how we will think about neural networks, specifically the LSTM model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX_WLYHrIM1_"
   },
   "source": [
    "-----\n",
    "\n",
    "## Introduction to Recursive Neural Networks (RNNs) \n",
    "\n",
    "\n",
    "The nice thing about spending time to understand the Fibonacci Sequence is that we can then `borrow the intuition` that we picked up to help us understand how the LSTM works. \n",
    "\n",
    "Different Recursive Neural Networks (RNNs) have this recursive loop in their architecture. The ML research community first created the following RNN model using the standard Fully-Connected Forward Feeding (FCFF) model: \n",
    "\n",
    "![](https://nerdthecoder.files.wordpress.com/2019/02/731df-0mrhhgabskajpbt21.png)\n",
    "\n",
    "`This type of RNN had severe limitations!` \n",
    "\n",
    "- It didn't have long-term memory capacity to learn long input sequences \n",
    "- It suffered from the [Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).\n",
    "\n",
    "In response to these limitations, the ML research community created the LSTM model, which ditched the FCFF architecture and started using the following architecture:\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "Wow! Ok! There's a lot going on here, isn't there? Well, don't worry, we are going to break this model down bit-by-bit so we can understand what is happening. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX_WLYHrIM1_"
   },
   "source": [
    "_____\n",
    "\n",
    "\n",
    "## Theory of LSTM\n",
    "\n",
    "One of the simplist and clearest explanations of the LSTM model can be found [**here!**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - a beautifully clear and concise explaination the model's archtecture and the mathematics. This link will serve as our main resouce for understanding how LSTMs work. \n",
    "\n",
    "Below are the equations for each of the gates that are explained in the article. \n",
    "\n",
    "Although, you will not be held responsible for the equations in any quiz, module assignment, or Sprint Challenge - it is still instructive to be exposed to them at least once.\n",
    "\n",
    "First thing to notice is that each gate equation (not the cell states) has the form of a perceptron. \n",
    "\n",
    "`Remember the perceptron?` It's the fundamental building block of neural networks - it's not going away! \n",
    "\n",
    "Once you understand that, it will hopefully become gradually clear that each gate is a perceptron with a different job to do. \n",
    "\n",
    "That's it. \n",
    "\n",
    "It's just 4 perceptrons, each with a different job to do. \n",
    "\n",
    "Fortunately, you already know about perceptrons (you built one from scratch in `Sprint 2 Module 1`). \n",
    "\n",
    "____\n",
    "\n",
    "### Gates in More Detail\n",
    "\n",
    "#### Forget Gate\n",
    "This neuron's job is to use the current input to learn what information the cell state should forget regarding long-term dependencies. \n",
    "\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1},x_t]~+~b_f)$$\n",
    "\n",
    "#### Input Gate\n",
    "This neuron's job is to use the current input to learn what new information to include in the cell state. \n",
    "\n",
    "\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1},x_t]~+~b_i)$$\n",
    "\n",
    "#### Candidate Cell State \n",
    "This neuron's job is to use the current input to create a candidate cell state.\n",
    "\n",
    "This new candidate cell state will be used to update the model's final cell state.\n",
    "\n",
    "$$\\tilde{C}_t = \\text{tanh}(W_C \\cdot [h_{t-1},x_t]~+~b_C)$$\n",
    "\n",
    "#### New Cell State\n",
    "This is where the candidate and old cell state are combined to create a new cell state.\n",
    "\n",
    "This is where output from the forget gate $f_t$ is used to scaled the old cell state\n",
    "\n",
    "- If $f_t$'s value is closer to 0.0, then less information from the previous cell state is retained.\n",
    "- If $f_t$'s value is closer to 1.0, then more information from the previous cell state is retained. \n",
    "\n",
    "\n",
    "This is also where the output of the input gate $i_t$ is used to scaled the candidate cell state. \n",
    "- If $i_t$'s value is closer to 0.0, then less information from the candidate cell state is retained\n",
    "- If $i_t$'s value is closer to 1.0, then more information from the candidate cell state is retained. \n",
    "\n",
    "Finally, you combine the two scaled cell states to form the new cell state of the model. \n",
    "\n",
    "It is $C_t$ that will be passed into the next training step and used by the output to make a final prediction. \n",
    "\n",
    "$$C_t = f_t*C_{t-1} + i_t*\\tilde{C}_t$$\n",
    "\n",
    "#### Output Gate\n",
    "This is where the actual output of the model is calcuated. \n",
    "\n",
    "The article denotes the model's pre-scaled output as $o_t$ and the scaled output as $h_t$. To be clear, it is $h_t$ that ultimately gets outputed as the model's final prediction. \n",
    "\n",
    "We are familiar with the notation $y$ to denote a model's prediction instead of using $h$. But they both mean the same thing - the model's final prediction. \n",
    "\n",
    "This neuron's job is to take the current input and make a prediction. \n",
    "\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1},x_t]~+~b_o)$$\n",
    "\n",
    "Next, the cell state is used to inform the final prediction. \n",
    "\n",
    "Recall that $o_t$ is output from a sigmoid activation function, so it's value is somewhere between 0 and 1. \n",
    "\n",
    "Which means that it is being used to scale $\\text{tanh}(C_t)$ which contains the current cell state. \n",
    "\n",
    "Recall the tanh curve and you'll see that tanh is scaling $C_t$ so that it's value lies between -1 and 1; this makes it possible to have positive and negative values for the model's output. Sigmoids don't allow for the posibility of negative values, but tanh does. \n",
    "\n",
    "$$h_t = o_t*\\text{tanh}(C_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX_WLYHrIM1_"
   },
   "source": [
    "_________\n",
    "\n",
    "### Today's Application of LSTMs\n",
    "\n",
    "So why are these cool? \n",
    "\n",
    "One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. \n",
    "\n",
    "Resources:\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "_____________\n",
    "\n",
    "\n",
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSytcRhoIM2A"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ti23G0gRe3kr",
    "outputId": "29e571de-a433-4d36-fd42-d204735bb807"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ti23G0gRe3kr",
    "outputId": "29e571de-a433-4d36-fd42-d204735bb807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/rhia/anaconda3/envs/U4-S3-DL/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhia/anaconda3/envs/U4-S3-DL/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# load in dataset \n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XWP9TNEM8-q",
    "outputId": "75929a58-5bfe-4541-f854-56fa24511b95"
   },
   "outputs": [],
   "source": [
    "# documentation on this data set here: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data\n",
    "# the values in the lists represents the token frequncy, so \"1\" means the most frequent token in the corpus \n",
    "# each list represents a movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK9k4UKJM9EC",
    "outputId": "cce49d58-d0de-40db-a76b-1a71e9a3d4f9"
   },
   "outputs": [],
   "source": [
    "# binary labels \n",
    "# 1 -> positive sentiment expressed in movie review\n",
    "# 0 -> negative sentiment expressed in movie review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_labels = len(np.unique(y_train))\n",
    "num_unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "c0awRJCnIM2G",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06de4d9b7986caa5aef6c537af138ee9",
     "grade": false,
     "grade_id": "cell-fb23c1d7d1168a73",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "cab328b7-9f86-4728-c62f-70deab2fa1f5"
   },
   "outputs": [],
   "source": [
    "# although there are some implmentations of LSTM models that can handle variable length samples, this is not one of those models\n",
    "# so we need to standardize the length of our movies\n",
    "# reviews that are longer than maxlen are truncated\n",
    "# reivewsd that are shorter than maxlen are padded with 0 (Or some other value that you provide)\n",
    "maxlen = 80\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a 1 hidden layer LSTM language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "QD_NjHw-pcJS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "200cc0a64ca234e836bf8fe83a553143",
     "grade": false,
     "grade_id": "cell-9c285c5d84213905",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "321e2638-8e0b-43cd-fd0f-26b0d41a94ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate the sequential class\n",
    "model = Sequential()\n",
    "\n",
    "# create the input layer(explicityly)\n",
    "# this layer identifies how many unique words appear in the dataset (ie 20000 for us)\n",
    "# and creates a fixed sized input layer vector to structure our incoming article vectors\n",
    "# so each article gets transformed into a 20,000 dim vector(20,000 for us)\n",
    "model.add(Embedding(max_features, 128))\n",
    "\n",
    "# create hidden layer\n",
    "# this is where we are processing the sequential part of the data\n",
    "model.add(LSTM(128, dropout=0.2,return_sequences=False))\n",
    "\n",
    "#create output layer\n",
    "# this is our classification layer.\n",
    "# we need an activation function with a partial derivative that can be used in Gradient descent\n",
    "# we use a sigmoid or softmax. here we use sigmoid as it is a binary classification task\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss = \"binary_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "98/98 [==============================] - 63s 612ms/step - loss: 0.4706 - accuracy: 0.7660 - val_loss: 0.3596 - val_accuracy: 0.8444\n",
      "Epoch 2/5\n",
      "98/98 [==============================] - 95s 973ms/step - loss: 0.2696 - accuracy: 0.8915 - val_loss: 0.3655 - val_accuracy: 0.8396\n",
      "Epoch 3/5\n",
      "98/98 [==============================] - 72s 735ms/step - loss: 0.1942 - accuracy: 0.9272 - val_loss: 0.4794 - val_accuracy: 0.8196\n",
      "Epoch 4/5\n",
      "98/98 [==============================] - 58s 588ms/step - loss: 0.1429 - accuracy: 0.9478 - val_loss: 0.4782 - val_accuracy: 0.8236\n",
      "Epoch 5/5\n",
      "98/98 [==============================] - 63s 646ms/step - loss: 0.1045 - accuracy: 0.9639 - val_loss: 0.5479 - val_accuracy: 0.8196\n"
     ]
    }
   ],
   "source": [
    "results_one_layer = model.fit(x_train, y_train,\n",
    "                      batch_size=256, \n",
    "                      epochs=5, \n",
    "                      validation_data=(x_test,y_test),workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a 1 hidden layer Bidirectional LSTM language model\n",
    "\n",
    "A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: **one taking the input in a forward direction**, and **the other in a backwards direction**. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence).\n",
    "\n",
    "![](https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "cKyGb4TzIM2O",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7058ca0b8f53f28530be9c93a8bef46",
     "grade": false,
     "grade_id": "cell-706b7be103484984",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "2cf2f039-ffc3-40e3-8810-e98b07240a3a"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_biLSTM = model.fit(x_train, y_train,\n",
    "                      batch_size=256, \n",
    "                      epochs=5, \n",
    "                      validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "RZx3Zs7tIM2Q",
    "outputId": "9d7d2ffd-25c0-4e0c-844c-d2de69ceef8c"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_list = np.arange(1,6)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.grid()\n",
    "plt.xticks(epoch_list)\n",
    "# results for 1-layer lstm model\n",
    "plt.plot(epoch_list, results_one_layer.history['loss'], \"--\", label=\"1 layer Train\")\n",
    "plt.plot(epoch_list, results_one_layer.history['val_loss'], \"--\", label = \"1 layer Test\")\n",
    "\n",
    "# results for 3-layer lstm model\n",
    "plt.plot(epoch_list, results_biLSTM.history['loss'], label=\"biLSTM Train \")\n",
    "plt.plot(epoch_list, results_biLSTM.history['val_loss'], label = \"biLSTM Test\")\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9Ps3CauIM2S"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pETWPIe362y"
   },
   "source": [
    "--------\n",
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sequential models to generate text data is a very popular application of recursive deep learning models. A couple of popular applications are [**chat bots**](https://hackernoon.com/deep-learning-chatbot-everything-you-need-to-know-r11jm30bc) and language translators such as [**google translate**](https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html). \n",
    "\n",
    "In order to properly build a chat bot or translater you need to use multiple lstm models in an encoder & decoder framwork known as a [**sequence 2 sequence model**](https://keras.io/examples/nlp/lstm_seq2seq/) .\n",
    "\n",
    "\n",
    "![](https://jeddy92.github.io/images/ts_intro/seq2seq_lang.png)\n",
    "\n",
    "Also, now a days, using a standard LSTM isn't enough. You also have to use a version of lstm seq2seq models known as [**transformers**](https://towardsdatascience.com/transformers-141e32e69591). Transformers give seq2seq models the capacity to pay attention to specific portions of the input sequence, the most relevent portion in order to make a prediction. Yes, that's right, humanity has figured out how to convert attention into an algorithm. Next stop, self-awareness! \n",
    "\n",
    "The above mentions of sequence 2 sequence models and transformers are for a larger contextual understanding of the landscape of language models and how LSTMs fit into this landscape. Although **we will cover the endcoder/decoder framework in a future lesson, transformers are outside the scope of Unit 4**. However, once you learn about LSTMs and encoder/decoder frameworks, you will have all necessary information to then go on and learn about transformers on your own. At that point, the only really new bit you'll be learning is the [**attention mechanism**](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f). \n",
    "\n",
    "\n",
    "As a first pass at text generation, we'll stick to standard LSTM models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BK-GrUGvIM2T"
   },
   "source": [
    "-----\n",
    "# Text Generation using LSTMs\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "q64qHEYIIM2U"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a custom data prep class that we'll be using \n",
    "from data_cleaning_toolkit_class import data_cleaning_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "MxcXsdsSIM2W",
    "outputId": "33551ac0-14c9-4832-d4ee-143c1f2e2a1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contributing columnist\\n\\nThe House is on fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When President Trump announced his decision to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Russian President Vladimir Putin speaks at a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>“The Queen’s Speech” is designed to acknowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Like an aging rock star, the president is now ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article\n",
       "0    Contributing columnist\\n\\nThe House is on fire...\n",
       "1    When President Trump announced his decision to...\n",
       "10   Russian President Vladimir Putin speaks at a s...\n",
       "100  “The Queen’s Speech” is designed to acknowledg...\n",
       "101  Like an aging rock star, the president is now ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load text data (articles)\n",
    "df = pd.read_json('https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/main/module1-rnn-and-lstm/wp_articles.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54b3f47f5eeed6cf3fd9732ac8abf1e5",
     "grade": false,
     "grade_id": "cell-292d1e2b08c74976",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b46715962d32c041b4849afdf6c87232",
     "grade": false,
     "grade_id": "cell-6a39513d81d87f1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Helper function to sample an index from a probability array\n",
    "    \"\"\"\n",
    "    # convert preds to array \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    # scale values \n",
    "    preds = np.log(preds) / temperature\n",
    "    # exponentiate values\n",
    "    exp_preds = np.exp(preds)\n",
    "    # this equation should look familar to you (hint: it's an activation function)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    # Draw samples from a multinomial distribution\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    # return the index that corresponds to the max probability \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"\"\n",
    "    Function invoked at end of each epoch. Prints the text generated by our model.\n",
    "    \"\"\"\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "\n",
    "    start_index = random.randint(0, len(text) - dctk.maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + dctk.maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        \n",
    "        x_dims = (1, dctk.maxlen, dctk.n_features)\n",
    "        x_pred = np.zeros(x_dims)\n",
    "        \n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, dctk.char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = dctk.int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this for on_epoch_end()\n",
    "text = \" \".join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0BFtoKUIM2x"
   },
   "outputs": [],
   "source": [
    "# create callback object that will print out text generation at the end of each epoch \n",
    "# use for real-time monitoring of model performance\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### Build Text Generating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "p7XeGd0a2MKi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6cb82e3e8cab149b063e8a7705aeae9",
     "grade": false,
     "grade_id": "cell-0b9d84be1c960668",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# build another model for our task for forecasting what text should follow from our seed string \n",
    "model = Sequential()\n",
    "# hidden layer 1 \n",
    "model.add(LSTM(128, \n",
    "               input_shape=(dctk.maxlen, dctk.n_features), # think of input_shape as implicitly declaring the input layer \n",
    "               return_sequences=True)) # set to true whenever using 2 or more LSTM layers \n",
    "# hidden layer 2 \n",
    "model.add(LSTM(64))\n",
    "# this is our output layer\n",
    "# recall that n_features = num of nodes in output layer \n",
    "model.add(Dense(dctk.n_features, \n",
    "                activation='softmax'))\n",
    "# notice that we are using categorical_crossentropy this time around - why?\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam')\n",
    "# fit the model\n",
    "# x and y are pretty large, consider sub-sampling\n",
    "model.fit(x[:1000], y[:1000],\n",
    "          batch_size=256,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBt5ugHKIM21"
   },
   "source": [
    "-------------\n",
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ger33u0CIM22"
   },
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DS16 LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "U4-S3-DL (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
