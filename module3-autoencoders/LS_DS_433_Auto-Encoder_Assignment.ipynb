{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Apply Auto-Encoders to Restore Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch the following 4 minute video on using an Auto-Encoder called DeepRemaster to restore old and grainy black and white video into higher quality, color video. \n",
    "\n",
    "It's a great video that sets up what we will be doing with our auto-encoder in this section. \n",
    "\n",
    "By the way here's the [**link to the white paper mentioned in this video**](https://arxiv.org/pdf/2009.08692.pdf) just in case you'd like to read through it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('EjVzjxihGvU', width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw in the video, the model architecture for DeepRemaster is a big complex beast of a model. And, since it is processing video data, you can image how much computational resources and time are required to properly train such a beastie. \n",
    "\n",
    "In this section, we will perform the same time of process but on a smaller and simpler scale - though the basic idea is the same. We will train an auto-encoder to restore damaged images. The beautiful and beneficial thing about performing the same basic restoration process but on a simpler problem is that we are able to understand the basic process without too many complications. Then, if this is something that you'd like to explore further, you can do so with the advantage of already having solved a similar and simpler version of the same problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# need these layers for 1st section\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "\n",
    "# need these layers for 2nd section\n",
    "from keras import layers\n",
    "from keras.layers import Reshape, Conv2DTranspose, Flatten\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Application of Auto-Encoders\n",
    "\n",
    "\n",
    "# Restore Damaged Mnist Dataset\n",
    "\n",
    "In this exercise, our tasks is to build a model that restores damaged images, namely damaged mnist digits. Because this is an exercise, we'll have to damage the images ourselves by adding randomly sampling a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Normalize Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mnist dataset \n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2acb9252321af4e3e54b41d29014fb85",
     "grade": false,
     "grade_id": "cell-19c2b92ae584afbd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# normalize pixel values between 0 and 1 for both the train and test set \n",
    "# save results to `x_train_norm` and `x_test_norm`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_train_norm.max() == 1.0, \"Did you normalized your training set?\"\n",
    "assert x_test_norm.max() == 1.0, \"Did you normalized your test set?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Original and Damaged Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see the first 10 original images from the test set \n",
    "plt.figure(figsize=(18,5))\n",
    "# helper function used to plot images \n",
    "for index in range(10):\n",
    "    plt.subplot(2,5, index+1)\n",
    "    plt.imshow(x_test_norm[index], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage Our Images\n",
    "\n",
    "We'll be damaging our images by adding noise to each pixel. We'll do this by sampling a normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea1701c7027782f925a0783ea2704987",
     "grade": false,
     "grade_id": "cell-66a89e39bed9bdc1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# index the .shape attribute to get the height of our images - save result to `n_rows`\n",
    "# index the .shape attribute to get the width of our images - save result to `n_cols`\n",
    "\n",
    "\n",
    "# move n_rows and n_cols into a tuple - save results to `size`\n",
    "\n",
    "\n",
    "# since our images are normalize we will sample normalize distribution with these parameters \n",
    "mean = 0.5\n",
    "stddev = 0.3\n",
    "\n",
    "# randomly sample from a normal distribution \n",
    "\n",
    "# creating the noisy train data by adding noise to x_train_norm - save result to `x_train_noisy`\n",
    "\n",
    "\n",
    "# creating the noisy test data by adding noise to x_test_norm - save result to `x_test_noisy`\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be seeing highly grainy images of hand written digits below. By normalizing our images and adding noise sampled from a normal distribution centered around 0.5, we have created damaged images. Now our next task is to build an auto-encoder that learns how to restore the original images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "# helper function used to plot images \n",
    "for index in range(10):\n",
    "    plt.subplot(2,5, index+1)\n",
    "    plt.imshow(x_test_noisy[index], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an Image Restoration Auto-Encoder\n",
    "\n",
    "You saw in the guided project that we can build an auto-encoder using Fully Connected Forward Feeding layers (FCFF) in Keras they're called Dense layers. Or we can build a Convolutional Auto-Encoder by using Cov2D and MaxPool2D layers. \n",
    "\n",
    "You are encouraged to experiment and build out an architecture of your choosing. However using an Convolutional Auto-Encoder is encouraged since that architecture is specifically designed for image data. \n",
    "\n",
    "Feel free to reference the guided project for examples of how to build an Auto-Encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce87bb78fa6719f0722c8c43ed00bebf",
     "grade": false,
     "grade_id": "cell-148b9e81e01868d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an input layer using Input() class\n",
    "\n",
    "# Create an encoder model \n",
    "\n",
    "# Create a decoder model \n",
    "\n",
    "# bring it all together by using the Model() class - save result to `restore_model`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complie model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62f177f85ce54e9d911e4cec112618a1",
     "grade": false,
     "grade_id": "cell-04d7ed3d4d8bfec0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# complie model \n",
    "# use `mse` for the loss \n",
    "# use `nadam` as the optimizer \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dcba2177c0c1618b24570e801871688",
     "grade": false,
     "grade_id": "cell-b0bf2468df6bcdac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# fit model \n",
    "# use `x_train_noisy` as the input train data \n",
    "# use `x_train_norm` as the output train data \n",
    "# use 3 epochs \n",
    "# if you have access to multiple processors, set parameter `workers` to N - 1\n",
    "# where N is the total number of processors that you have \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained model to restore damaged images \n",
    "\n",
    "Now that our model `restore_model` has been trained. We can use it to restore damaged images. \n",
    "\n",
    "- Pass in `x_test_noisy` into the `.predict()` method - save results to `restored_imgs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54b3a700d3a6a4e35454d55541861380",
     "grade": false,
     "grade_id": "cell-49247f9edc4892b0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Damaged Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "# display the first 10 images from `x_test_noisy`\n",
    "for index in range(10):\n",
    "    plt.subplot(2,5, index+1)\n",
    "    plt.imshow(x_test_noisy[index], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Restored Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "# display the first 10 images from `restored_imgs`\n",
    "for index in range(10):\n",
    "    plt.subplot(2,5, index+1)\n",
    "    plt.imshow(restored_imgs[index], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "Ok - so it worked! We were able to restored damaged images by using an auto-encoder as a restoration model. Apply the same essential idea as in the video from the top of the notebook. \n",
    "\n",
    "\n",
    "You might be thinking to yourself \"Wait a minute?! We create the problem, then create a solution to the problem? Doesn't this feel a little circular?\"\n",
    "\n",
    "If you were thinking that then, yeah, you'd be right. \n",
    "\n",
    "However, in practice what you would be doing is finding a non-deep learning solution for restoring a small amount of images that you can then use as a training set. This might be a manual by-hand solution or by some other means. Then once you have enough copies of damaged and restored pairs, then you can create a training set, train a model, then from that point on the model will do the restoration in an automated fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Exercise 2: Introduction to Variational Auto-Encoders\n",
    "\n",
    "\n",
    "\n",
    "We are going to take the Standard Auto-Encoder framework that we have been playing with and change two things about it. In order to get started on how to understand these changes you are first going to watch the provided video, then we'll break down the important bits in the notebook. \n",
    "\n",
    "Although you are highly encouraged to experiment with the code provided, you will not be asked to fill in any missing code in this section. \n",
    "\n",
    "**This is an exercise in reading comprehension.** \n",
    "\n",
    "There are questions at the end of this section that test your reading comprehension of the technical knowledge that you will be exposed to shortly.\n",
    "\n",
    "\n",
    "## First some Theory\n",
    "\n",
    "Watch the following video to get a crash course on the theory of Variational Auto-Encoders. \n",
    "\n",
    "The video has a lengthy introduction, so the VAE section actually **starts at about 5:40 and ends at about 9:06**. Afterwards, he goes on to discuss a more sophisticated version of VAE that is outside the scope of today's assignment.\n",
    "\n",
    "Don't stress out if you don't understand what he is saying. If fact take note of the following: \n",
    "\n",
    "- Watch the VAE video 2 - 3 times and absorb as much information as you can\n",
    "- We will unpack the important bits later in the notebook \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('9zKuYvjFFS8', width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lh3.googleusercontent.com/proxy/9StAnnRiBvF4rNIMEXj2Qc5kGvWHQu7H6wOqxYI9wAPCN3Jy8JUE4awTyslXvFO2Etb2-yY8xgFvAH0zNMx8BUQmQ0Ca2FwgOw)\n",
    "\n",
    "Having watched the video, you'll see that there's a lot of technical details but we really only care about two things:\n",
    "\n",
    "- The introduction of the **z_mean** and  **z_log_var** vectors \n",
    "- The 2 part loss function with the **Reconstruction loss** and **KL loss** components \n",
    "\n",
    "Let's focus on the **z_mean** and  **z_log_var** vectors for now and we'll come back to the loss function later. \n",
    "\n",
    "Notice that the Variational Auto-Encoder is our standard Auto-Encoder plus the **z_mean** and  **z_log_var** vectors in the middle. \n",
    "\n",
    "Our standard Auto-Encoders also have **latent vectors** - it's simply the output of the last encoding layer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Normal Distribution in Latent Space\n",
    "\n",
    "### Latent Space \n",
    "\n",
    "First of all **what is Latent Space?**\n",
    "\n",
    "$$\\textbf{Latent Space means a representation of compressed data.}$$\n",
    "\n",
    "Simple right? \n",
    "\n",
    "Hmm..maybe too simple. \n",
    "\n",
    "Maybe you want that definition to be unpacked and explained a little more. \n",
    "\n",
    "In a **Linear Algebra** context Latent Space is **representing high dimensional data in a low dimensional space.** \n",
    "\n",
    "\n",
    "Still want a deeper explanation? \n",
    "\n",
    "Then you're in luck! You can [**read this article to get a deeper explaination of Latent Space**](https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d#:~:text=The%20latent%20space%20is%20simply,representations%20of%20data%20for%20analysis.) (only if you want to of course). \n",
    "\n",
    "\n",
    "### Latent N-dimensional Distribution \n",
    "\n",
    "![](https://theaisummer.com/assets/img/posts/Autoencoder/vae.png)\n",
    "\n",
    "\n",
    "The image above shows the architecture of a Variational Auto-Encoder. Notice how it almost looks exactly like a standard Auto-Encoder? It almost is expect for those latent vectors in the middle. \n",
    "\n",
    "The basic idea is that instead of representing the latent vector as simply another vector: as the output vector from the last layer of the encoder model. We are going to **represent the latent vector (call it Z) as having two components: a mean vector component and a variance vector component.** \n",
    "\n",
    "![](https://bookdown.org/phamtrongthang123/notebookCEVAE/imgs/2020-04-12-10-49-37.png)\n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{\\mu} + \\mathbf{\\sigma} \\bigodot \\mathbf{\\epsilon}$$\n",
    "\n",
    "Where $\\epsilon$ is sampled from a normal distribution \n",
    "\n",
    "$$\\epsilon \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2}) ~=~ \\mathcal{N}(0, 1) $$\n",
    "\n",
    "And $\\bigodot$ means matrix product. \n",
    "\n",
    "We know that **mean** and **variance** are just numbers, single floating point values, right? So why do we need vectors for these distribution parameters?\n",
    "\n",
    "Because there is such a thing as an [**N-dimensional Normal distribution**](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). So far in the program, you have only been exposed to univariate (i.e. 1-dimensional) probability distributions (i.e. $P(x)$), which come up a lot in practice so that's why they are so heavily focused on. However, multi-dimensional probability distributions do exists and that is what we will be playing with today. In fact, look at the VAE image and you'll see a 2-dimensional Normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"\n",
    "    Uses (z_mean, z_log_var) to sample a z vector from the latent N-dimensional Normal distribution\n",
    "    i.e. Z is the vector encoding a digit.\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # recall from the video that z_mean and z_log_var are vectors \n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        \n",
    "        # sample from an N-dimensional normal distribution\n",
    "        # epsilon is given shape (batch, dim) because we are adding it to z_mean which has shape (batch, dim)\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        \n",
    "        # output of tf.exp() is a vector \n",
    "        simga = tf.exp(0.5 * z_log_var)\n",
    "        \n",
    "        # this is our hideen latent vector made up of a mean and variance vector \n",
    "        # variance vector is scaled by epsilon, which is sampled from a normal distribtuion \n",
    "        # the the video guy said \"stocastic\" in reference to epsilon, he meant random \n",
    "        Z = z_mean + simga * epsilon\n",
    "        \n",
    "        # return hidden latent vector \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Variance Auto-Encoder model\n",
    "\n",
    "We are going to build the Encoder model, followed by the Decoder model. Then bring it all together by using Keras's Model API for building models, just like we did in the guided project. \n",
    "\n",
    "## Build Encoder Model\n",
    "\n",
    "Note that the there is nothing special about the the architecture we are using. You can experiment with different architectures after you've gone through the notebook once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall from the video that the more dimensions that our latent vector has\n",
    "# the better the results of our model \n",
    "latent_dim = 2\n",
    "\n",
    "# shape of our input data\n",
    "# we are creating our input layer using Keras's Input() class\n",
    "# the only thing that input layers really do is define the dimensionality of the input data for the model\n",
    "encoder_inputs = Input(shape=(28, 28, 1))\n",
    "\n",
    "# these are the hidden layers of our encoder model \n",
    "# `x` is the output from each layer \n",
    "# recall that the data is in the shape of a matrix, hence Conv2D - 2D as in 2-dimensional \n",
    "x = Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# now flatten data matrix into data vector \n",
    "x = Flatten()(x)\n",
    "# pass data vector into FCFF layer \n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "# recall that ordinarly the output of the last encoding layer is the latent vector \n",
    "# but here we are creating two output layers for our encoder - one for the mean and one for the log variance \n",
    "# returns a 2-dim mean vector\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "# returns a 2-dim log variance vecotr \n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "# pass mean and variance vector into Sampling class in order to create the Z vector,  Z = mean + var * epsilon\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "# ok, now let's put it all together \n",
    "# this is our encoder model \n",
    "# inputs are the original images\n",
    "# outputs are the Z vectors: mean, log variance, and the complete Z, i.e. vector Z = mean + var * epsilon\n",
    "encoder = Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Decoder Model\n",
    "\n",
    "Now we can create the decoder model.\n",
    "\n",
    "There is a new layer here that we should talk about, it's the **Conv2DTranspose** layer. \n",
    "\n",
    "This is also known as a **DeConvolutional layer**. It's like the Convolutional layer that we learned about in Sprint 3 Module 2 but it \"moves\" in the opposite direction. \n",
    "\n",
    "\n",
    "Huh? Ok, let me explain. \n",
    "\n",
    "### DeConvolution\n",
    "\n",
    "![](https://miro.medium.com/max/1086/1*AbCrAqPBfkqGRdhKtiZQqA.png)\n",
    "\n",
    "On the **left hand side** we see what a **Convolution** looks like (this should look familiar). The layer is taking the original image (or feature map) on the left and applying a convolution, the result is a feature map on the right with a smaller dimensionality. \n",
    "\n",
    "On the **right hand side** we see what a **DeConvolution** looks like. The layer is taking the feature map (the output of a convolution layer) and attempting to rebuild the original image. \n",
    "\n",
    "Recall that a  **Convolution**  slides a weight matrix on an image in order to reduce the dimensionality and to create features which are stored on a matrix (or volume which is a stacking of matrices) call **Feature Maps.** \n",
    "\n",
    "The **DeConvolution**  layer starts with **Feature Maps** and tries to create the original image. \n",
    "\n",
    "Hence **DeConvolution** is like **Convolution**  just moving in the opposite direction. \n",
    "\n",
    "For a deeper explanation [**check out this article**](https://medium.com/@marsxiang/convolutions-transposed-and-deconvolution-6430c358a5b6)\n",
    "\n",
    "Also don't forget about the [**Keras docs on this layer**](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input layer to our decoder has the same dimensionality as the latent vector\n",
    "# because the latent vector is the input to the decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,))\n",
    "\n",
    "# these are the hidden layers of our decoder \n",
    "# the data at this point is in a vector, the Z latent vector \n",
    "x = Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# reshape vector into a matrix\n",
    "x = Reshape((7, 7, 64))(x)\n",
    "# increase size of matrix by using DeConvolutional layers\n",
    "# notice that the number of feature maps (i.e. the first parameter values) are mirror images\n",
    "# of the number of feature maps from the Conv2D layers in the Encoder\n",
    "# we saw this mirroring in the auto-encoders we built in the guided project \n",
    "x = Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# this is the final layer in out decoder\n",
    "# therefore this layer outputs the reconstruction of the original image \n",
    "decoder_outputs = Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "# this is our decoder model \n",
    "decoder = Model(inputs=latent_inputs, outputs=decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Encoder & Decoder into a Variational Auto-Encoder \n",
    "\n",
    "Now that we have created the Encoder and Decoder models, we can put it all together to finally create out Variational Auto-Encoder. In order to keep our code tidy, let's define the VAE model as a class. \n",
    "\n",
    "**Side Note:** \n",
    "\n",
    "You've probably notice how much code needs to written just to run a gridsearch experiment in Sprint 2 and now all the code that one needs to write just to create a deep learning VAE model. Yeah, welcome to Deep Learning folks. DL quickly becomes an engineering problem. Something to consider if you're considering spending a lot of time building DL models. To that point, it's actually a good thing that you're exposed to all of this now so that you can get an honest and proper exposure to what this kind of work is really like.\n",
    "\n",
    "\n",
    "**property decorator function**\n",
    "\n",
    "You might have noticed the python decorator above the **metrics()** method in the class below. \n",
    "\n",
    "```python\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"\n",
    "        Returns all loses in a list\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "```\n",
    "\n",
    "If you're not familiar with this decorator, [**read this**](https://www.programiz.com/python-programming/property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        \"\"\"\n",
    "        This class build a Variational Auto-Encoder. It accepts an Encoder and Decoder model as input. \n",
    "        \n",
    "        Note\n",
    "        ----\n",
    "        This VAE class is inheriting Keras's Model API so that it can use the Model class methods \n",
    "\n",
    "        \"\"\"\n",
    "        # how python 3 handels inheritance \n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        # set encoder model as class attribute\n",
    "        self.encoder = encoder\n",
    "        # set decoder model as class attribute \n",
    "        self.decoder = decoder\n",
    "        # set mean function as class attribute - this calculates the total loss\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        # set mean function as class attribute - this calculates the reconstruction loss\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        # set mean function as class attribute - this calculates the kl loss\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"\n",
    "        Returns all loses in a list\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Training our model via gradient descent and back-propagation \n",
    "        \"\"\"\n",
    "        \n",
    "        # we used tf.GradientTape() in Sprint 2 Module 2 to run Gradient Descent from scratch \n",
    "        with tf.GradientTape() as tape:\n",
    "            # pass input data into encoder model \n",
    "            # output of encoder model is the hidden state distribution parameters and hidden state vector \n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            \n",
    "            # pass hidden state vector into decoder model \n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            # calculate the reconstruction loss \n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # calculate the kl loss\n",
    "            #                (1 + z_simga   - (z_mean)^2        - e^(z_simga) ) \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            # recall that we used tf.reduce_sum() in Sprint 2 Module 4 assignment \n",
    "            # it takes the sum of the vector components \n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            \n",
    "            # calculate the total loss by adding the two loss components \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        # now that we have calculated the loss function, we can perform Gradient Descent\n",
    "        # we are passing in the loss function and the weights that we want to update via Gradeint Descent \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        # log the total loss\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        \n",
    "        # log the reconsgrution loss \n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        \n",
    "        # log the kl loss \n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        # return all the losses in a dictionary \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function \n",
    "\n",
    "Ok , let's return to the loss function that the VAE uses. \n",
    "\n",
    "From the video, and by reading the code in the VAE class up above, we can see that there are two components to the Loss Function. \n",
    "\n",
    "$$\\textbf{Total loss = binary crossentropy + KL Divergence}$$\n",
    "\n",
    "Fortunately, we covered the **CrossEntorpy** loss function in Sprint 2 Module 3, so we're already familiar with that part. But now there's this so called **KL Divergence** loss. So here's the short explanation. \n",
    "\n",
    "\n",
    "Let's say that we have a distribution of points but we don't actually know what kind of distribution it is. Is it a normal distribution, a binomial distribution, a Possian distribution, or something else entirely? We don't know. \n",
    "\n",
    "So what we can do is form the hypothesis that the distribution of points is a normal distribution. How good is our hypothesis? Well, we can use the KL Divergence to measure how similar the distribution of points is to a normal distribution. The lower the **KL Divergence** score (i.e. the lower the divergence between the two distributions), the better the match. \n",
    "\n",
    "\n",
    "So armed with that knowledge let's return to our loss function and break it down. \n",
    "\n",
    "$$\\textbf{Total loss = binary crossentropy + KL Divergence}$$\n",
    "\n",
    "**binary crossentropy** is responsible for accounting for the quality of the model's predictions, i.e. (y_pred vs y_true). \n",
    "\n",
    "\n",
    "**KL Divergence** is responsible for making sure that the latent probability distribution that our model learns is in fact a normal distribution. And to the extent that the latent probability distribution is not normal, the score will be non-zero. If the learned latent probability distribution is perfectly Normal (or nearly perfectly Normal), then the the score will be zero or near zero. \n",
    "\n",
    "### Further Readings\n",
    "\n",
    "For a deeper explanation of **KL Divergence** using a simple and concrete example [**check out this article.**](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8). \n",
    "\n",
    "For a deeper explanation of **KL Divergence** in the context of a VAE [**check out this article**](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mnist data set\n",
    "# we don't care about the Y labels since this isn't a supervised classification task \n",
    "# this is an unsupervised variational auto-encoder \n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# We don't care about the distinction between train and test sets here\n",
    "# so combine all data \n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "\n",
    "# change shape and normalize pixel values between 0 and 1\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255.\n",
    "\n",
    "# instantiate a Variational Auto-Encoder model \n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "# complie the model \n",
    "vae.compile(optimizer=keras.optimizers.Nadam())\n",
    "\n",
    "# train the model weights \n",
    "vae.fit(mnist_digits, epochs=30, batch_size=128, workers=10)\n",
    "# if you have access to multiple processors, set parameter `workers` to N - 1\n",
    "# where N is the total number of processors that you have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Results \n",
    "\n",
    "Now we will plot our modeling results in order to understand them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_digits(digit_size, n, grid_x, grid_y, figure):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function samples from the 2-Dimensional latent space Normal probability distribution \n",
    "    that our model as learned during training.\n",
    "    \"\"\"\n",
    "\n",
    "    ## This code samples images from the 2D Latent Space Normal Probability Distribtuion ##\n",
    "    # iterate through values in the y-axis\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        # iterate through values in the x-axis\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            # create a z vector using those (x,y) corrdinates \n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            # pass in that z vector into th decoder model\n",
    "            # notice that we aren't passing in an image but rather a 2D coordinate, but a 2D coordinate of what?\n",
    "            # we are sampling from the latent space 2D Normal distribution\n",
    "            # we pass in a 2D corrdinate, and the decoder passes back whatever image exists at that coordinate\n",
    "            x_decoded = vae.decoder.predict(z_sample)\n",
    "            # reshape the image that the decoder returned \n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            # store digit image in list for plotting\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "                    \n",
    "    return figure\n",
    "\n",
    "\n",
    "def plot_latent_space(vae, n=30, figsize=15):\n",
    "    \"\"\"\n",
    "    This function plots the latent space Normal probability distribution that our model as learned during training. \n",
    "    Read through the comments to learn how. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vae: keras object\n",
    "        Trained Variational Auto-Encoder\n",
    "        \n",
    "    n: int\n",
    "        number of tick marks on the x and y axis \n",
    "    \"\"\"\n",
    "    \n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    \n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    # i.e. we are defining the tick marks of the plot\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "    \n",
    "    # use our trained VAE to generate digit images for us \n",
    "    figure = generate_digits(digit_size, n, grid_x, grid_y, figure)\n",
    "    \n",
    "    # plot all the digit images that we sampled \n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    \n",
    "    \n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Latent Space \n",
    "\n",
    "Take a look at the plot below. How would you describe it?\n",
    "\n",
    "Our VAE has learned a Latent Space 2-Dimensional Normal probability distribution. \n",
    "\n",
    "Depending on your (x,y) coordinate in this space, the distribution has a probability of generating an image for you. Yes, that's right, this model's decoder will generate images for you. All you have to do is provide an (x,y) coordinate. This is true because we have kind of model that is known as a [**Generative Model**](https://towardsdatascience.com/generative-deep-learning-lets-seek-how-ai-extending-not-replacing-creative-process-fded15b0561b). \n",
    "\n",
    "Oppose to what? \n",
    "\n",
    "### Fundamental difference between Discriminative models and Generative models\n",
    "\n",
    "**Discriminative models** learn the (hard or soft) boundary between classes\n",
    "\n",
    "    - Logistic Regression \n",
    "    - Random Forest\n",
    "    - SVM\n",
    "    - FCFF, LSTM, and CNN neural networks \n",
    "    \n",
    "**Generative models** model the distribution of individual classes\n",
    "\n",
    "    - Naive Bayes classifier\n",
    "    - Variational Auto-Encoder \n",
    "    - Generative Adversarial Networks \n",
    "\n",
    "\n",
    "The plot below shows the distribution of images that our model has learned. Interesting, isn't it?\n",
    "It's interesting to see how one digit slowly morphs into another digit. \n",
    "\n",
    "Make sure to click on the link above to read about some really cool applications of generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Label Clusters\n",
    "\n",
    "The plot below shows us the distribution of digit labels. The VAE passes the original digit image through encoder and plots the **z_mean** vector. We can see that the model thinks that 0's and 1's are spatially dissimilar from 8's and 9's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "\n",
    "plot_label_clusters(vae, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Time for Questions\n",
    "\n",
    "In your own words, answer as many of the following questions as you can. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "\n",
    "In a couple of simple sentences, **describe the differences between a Standard Auto-Encoder and a Variational Auto-Encoder.**\n",
    "\n",
    "Think how you would describe the differences to a non-data scientist over a couple of coffee. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**\n",
    "\n",
    "Now in technical detail, **describe the differences between a Standard Auto-Encoder and a Variational Auto-Encoder.** \n",
    "\n",
    "Think how you would describe the differences in a technical interview. Imagine the interviewer hasn't asked you for in-depth technical details like the equation of the KL Divergence loss function, so you don't want to get lost in the weeds of technical detail. But you do want your explanation to show that you could talk about the technical details if he asked you a follow up question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**\n",
    "\n",
    "How would you describe the technical details **KL Divergence** loss function to a fellow data science student outside the context of a VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:**\n",
    "\n",
    "How would you describe the technical details of the entire loss function **Binary CrossEntorpy** and **KL Divergence** loss functions in the context of a VAE in a technical interview?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37  (Python3)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
