{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gGVhV2kHQg0"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNBWUEQ-HQg1"
   },
   "source": [
    "# Convolutional Neural Networks (Prepare)\n",
    "\n",
    "> Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. *Goodfellow, et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UbEmkUqHQg1"
   },
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe convolution and pooling\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a convolutional neural network to a classification task\n",
    "- <a href=\"#p3\">Part 3: </a>Use a pre-trained convolution neural network for image classification\n",
    "\n",
    "Modern __computer vision__ approaches rely heavily on convolutions as both a `dimensionality reduction and feature extraction method`. Before we dive into convolutions, let's talk about some of the common computer vision applications: \n",
    "\n",
    "\n",
    "\n",
    "* Classification [(Hot Dog or Not Dog)](https://www.youtube.com/watch?v=ACmydtFDTGs)\n",
    "* Object Detection [(YOLO)](https://www.youtube.com/watch?v=MPU2HistivI)\n",
    "* Pose Estimation [(PoseNet)](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)\n",
    "* Facial Recognition [Emotion Detection](https://www.cbronline.com/wp-content/uploads/2018/05/Mona-lIsa-test-570x300.jpg)\n",
    "* and *countless* more \n",
    "\n",
    "We are going to focus on classification and pre-trained classification today. What are some of the applications of image classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "hP6M3V-YHQg2",
    "outputId": "446a957f-8d62-42f6-81c9-1191c1f4bf46"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('MPU2HistivI', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly00NdnJHQg5",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Convolution & Pooling (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06h_3Bj3HQg6",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "Just like perceptrons and Fully Connected neural networks, CNNs are also inspired by biology - specifically, the receptive fields of the visual cortex.\n",
    "\n",
    "\n",
    "Put roughly, in a real brain the neurons in the visual cortex **specialize** to be receptive to certain regions, shapes, colors, orientations, and other common visual features. \n",
    "\n",
    "In a sense, the very structure of our cognitive system transforms raw visual input, and sends it to neurons that specialize in handling particular subsets of it.\n",
    "\n",
    "CNNs imitate this approach by applying a convolution. A convolution is an operation on two functions that produces a third function, showing how one function modifies another. Convolutions have a [variety of nice mathematical properties](https://en.wikipedia.org/wiki/Convolution#Properties) - commutativity, associativity, distributivity, and more. Applying a convolution effectively transforms the \"shape\" of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "4M5hnthyHQg6",
    "outputId": "0b6e8acc-d158-47fc-9265-0c67bbd73c79"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('IOHayh06LJ4', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw3X-_GsHQg8",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's try to do some convolutions and pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krm3RupmHQg9"
   },
   "source": [
    "### Convolution\n",
    "\n",
    "Consider blurring an image - assume the image is represented as a matrix of numbers, where each number corresponds to the color value of a pixel.\n",
    "\n",
    "![](https://lambdaschool-data-science.s3.amazonaws.com/images/Unit4/Sprint2/Module2/Screen+Shot+2020-02-25+at+10.27.17+AM.png)\n",
    "\n",
    "*Image Credits from __Hands on Machine Learning with Sckit-Learn, Keras & TensorFlow__*\n",
    "\n",
    "\n",
    "Helpful Terms:\n",
    "- __Filter__: The weights (parameters) we will apply to our input image.\n",
    "- __Stride__: How the filter moves across the image\n",
    "- __Padding__: Zeros (or other values) around the  the input image border (kind of like a frame of zeros). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsAcbKvoeaqU",
    "outputId": "bac22f49-8e0b-4f50-9cf1-0ea7e2d650b9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color, io\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "austen = io.imread('https://dl.airtable.com/S1InFmIhQBypHBL0BICi_austen.jpg')\n",
    "austen_grayscale = rescale_intensity(color.rgb2gray(austen))\n",
    "austen_grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "KN-ibr_DhyaV",
    "outputId": "2ee35e86-8924-4de0-e9b4-aff8eaa5b514"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_grayscale, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QopB0uo6lNxq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "# this a convolutional filter (i.e. a weight matrix) that perserves the horizontal lines in an image\n",
    "# during lecture I reasoned outloud how this matrix is actually able to preserve the horizontal lines\n",
    "# CHALLENGE: try to convince yourself that you understand how these matrices are able to do what they do  \n",
    "horizontal_edge_convolution = np.array([[1,1,1,1],\n",
    "                                        [0,0,0,0],\n",
    "                                        [0,0,0,0],\n",
    "                                        [-1,-1,-1,-1]])\n",
    "\n",
    "# this a convolutional filter (i.e. a weight matrix) that perserves the vertical lines in an image \n",
    "vertical_edge_convolution = np.array([[1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1]])\n",
    "\n",
    "# this a convolutional filter (i.e. a weight matrix) that perserves the diagonal lines in an image \n",
    "diag_edge_convolution = np.array([[1,0,0,-1],\n",
    "                                  [0,1,-1,0],\n",
    "                                  [0,-1,1,0],\n",
    "                                  [-1,0,0,1]])\n",
    "\n",
    "# Doc for nd.convolve: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.convolve.html\n",
    "austen_edges_vert = nd.convolve(austen_grayscale, vertical_edge_convolution)\n",
    "austen_edges_horz = nd.convolve(austen_grayscale, horizontal_edge_convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "-LwEpFW1l-6b",
    "outputId": "4d6006f1-409f-4d09-a077-5d0cc09da8f6"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_edges_vert, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "8TyaToNjHQhH",
    "outputId": "9eb3bbde-d847-4c3c-d837-9fc922aaa8df"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_edges_horz, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Convolutional Neurons \n",
    "\n",
    "Here's the explanation of the convolution calculation that we just did using [**scipy.ndimage.convolve**](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.convolve.html).\n",
    "\n",
    "\n",
    "In that link the calcualtion is given by $C_i = \\sum_j{I_{i+k-j} W_j}$, where $C_i$ is the value of the convolution. \n",
    "\n",
    "Don't worry about the various indicies (that's not important) what's import is the general structrue of the equation. \n",
    "\n",
    "Does it look familar to you? \n",
    "\n",
    "No? Let's re-express the sum as a vector product. \n",
    "\n",
    "How about now?\n",
    "\n",
    "$$\\mathbf{w} \\cdot \\mathbf{x}$$\n",
    "\n",
    "How about now?\n",
    "\n",
    "$$\\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "How about now?\n",
    "\n",
    "$$\\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "How about now?\n",
    "\n",
    "$$y~=~\\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "The convolutional layer is populated with artifical neurons (perceptrons) that specialize in processing visual information - but they are still perceptrons as we can see from the math. \n",
    "\n",
    "Now **scipy.ndimage.convolve** doesn't use an activation function or a bias term, but the convolutional layers that we'll be using in our CNNs do have a bias term and (optionally) an activation function. \n",
    "\n",
    "\n",
    "Do you see how ubiquitous that perceptron equation is? This is why we took the time to learn it in Sprint 2.\n",
    "\n",
    "Again, **the perceptron is the fundamental building block of a neural network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Maps \n",
    "\n",
    "![](https://www.researchgate.net/profile/Mehdi-Cherti/publication/326412238/figure/fig29/AS:648874402271233@1531715121461/Example-of-feature-hierarchy-learned-by-a-deep-learning-model-on-faces-from-Lee-et-al.png)\n",
    "\n",
    "The matrix that is created by the convolutions are called **Feature Maps**. \n",
    "\n",
    "Feature maps are matrices that contain the output of a convolutional layer. \n",
    "\n",
    "These feature maps contain, well, features. \n",
    "\n",
    "Each convolutional layer is performing both dimensionality reduction and feature engineering. \n",
    "\n",
    "In order to understand the above image, let's focus on the feature engineering part. \n",
    "\n",
    "**1st layer's job** is to detect lines/edges from the raw pixel values, then pass those lines (features that were created by the convolutional neurons) to the next layer. \n",
    "\n",
    "**2nd layer's job** is to accept the lines and combine them (like lego blocks) to create new features with them, this time parts of an object. Those parts are then passed forward to the next convolutional layer. \n",
    "\n",
    "**3rd layer's job** is to accept the parts of an object and put them together (like lego blocks) and create even more complex features, like a person's face, a car, a building, an animal, any object contained in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDaTY2uSHQhJ"
   },
   "source": [
    "------\n",
    "\n",
    "### Pooling Layer\n",
    "\n",
    "![](https://lambdaschool-data-science.s3.amazonaws.com/images/Unit4/Sprint2/Module2/Screen+Shot+2020-02-25+at+10.26.13+AM.png)\n",
    "\n",
    "*Image Credits from __Hands on Machine Learning with Sckit-Learn, Keras & TensorFlow__*\n",
    "\n",
    "We use Pooling Layers to reduce the dimensionality of the feature maps. We get smaller and smaller feature set by apply convolutions and then pooling layers. \n",
    "\n",
    "Let's take a look very simple example using Austen's pic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "eAePA6xCHsn7",
    "outputId": "387d709c-723c-41bc-b54d-68a41f6d1faf"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_grayscale, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "PUcpIHm_HQhK",
    "outputId": "c153b7de-1596-4067-c4f2-1b0bdeef64dc"
   },
   "outputs": [],
   "source": [
    "from skimage.measure import block_reduce\n",
    "\n",
    "reduced = block_reduce(austen_grayscale, (2,2), np.max)\n",
    "plt.imshow(reduced, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the motivation behind the pooling layer is to reduce the number \n",
    "# of trainable weights from potentialy billions down to millions or 100Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T18SB3PMHQhL",
    "outputId": "919c113f-9f6a-402f-d859-029b6de0e6fd"
   },
   "outputs": [],
   "source": [
    "# by using a MaxPooling of (2,2) we are able to reduce the size of our image by a factor of 2 with out noticably lossing any important information\n",
    "# we still preserve the light contrast on his face\n",
    "# we still preserve the lines on his face\n",
    "# there is a spot on the left side (our left) of his upper mouth that gets a smoothed out a bit \n",
    "reduced.shape, austen_grayscale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0IO7nP9HQhN"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to be able to describe convolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qty8NQlnHQhN"
   },
   "source": [
    "---------\n",
    "\n",
    "# CNNs for Classification (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QK6ZUOQZHQhO",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOep4ugw8coa"
   },
   "source": [
    "### Typical CNN Architecture\n",
    "\n",
    "![A Typical CNN](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/800px-Typical_cnn.png)\n",
    "\n",
    "The first stage of a CNN is, unsurprisingly, a convolution - specifically, a transformation that maps regions of the input image to neurons responsible for receiving them. The convolutional layer can be visualized as follows:\n",
    "\n",
    "![Convolutional layer](https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png)\n",
    "\n",
    "The red represents the original input image, and the blue the neurons that correspond.\n",
    "\n",
    "As shown in the first image, a CNN can have multiple rounds of convolutions, [downsampling](https://en.wikipedia.org/wiki/Downsampling_(signal_processing)) (a digital signal processing technique that effectively reduces the information by passing through a filter), and then eventually a fully connected neural network and output layer. Typical output layers for a CNN would be oriented towards classification or detection problems - e.g. \"does this picture contain a cat, a dog, or some other animal?\"\n",
    "\n",
    "\n",
    "#### A Convolution in Action\n",
    "\n",
    "![Convolution](https://miro.medium.com/max/1170/1*Fw-ehcNBR9byHtho-Rxbtw.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA_8-zDNHQhP"
   },
   "source": [
    "------\n",
    "# Build a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWKwH7DIHQhP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6HR5Jz4HQhR"
   },
   "outputs": [],
   "source": [
    "# load in our color images\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "O6HR5Jz4HQhR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a42ca34eafe33ce59fa58825f161789b",
     "grade": false,
     "grade_id": "cell-caf1d29aa68b3ca1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "# this is done by dividing by the max pixel value \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "o8qu6KtQHQhT",
    "outputId": "23b7d1bb-f237-47c9-a11d-e14b26f7ba88"
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', \n",
    "               'automobile', \n",
    "               'bird', \n",
    "               'cat', \n",
    "               'deer',\n",
    "               'dog', \n",
    "               'frog', \n",
    "               'horse', \n",
    "               'ship', \n",
    "               'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "![](https://miro.medium.com/max/891/0*jGB1CGQ9HdeUwlgB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Yh04xEGHQhV",
    "outputId": "d53d5afd-5218-4da4-9896-34b9c925321d"
   },
   "outputs": [],
   "source": [
    "# this is a Rank 3 tensor \n",
    "# another way of thinking about this array is that it is 3-dim\n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLsZ6GM_IhMV",
    "outputId": "43b38f01-7540-453c-b933-cb463af514e3"
   },
   "outputs": [],
   "source": [
    "# this is a Rank 4 tensor \n",
    "# another way of thinking about this array is that it is 4-dim\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "### Build a CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "095ada94d0369ea63285e68ff14b9356",
     "grade": false,
     "grade_id": "cell-5a304de2902938ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "aa5ZcbSWHQha",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4389043225e78ea7e0673db30f82ff1",
     "grade": false,
     "grade_id": "cell-5e50d5c53fbd3f6c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "34cccff5-803a-4ca1-cd06-d1c48ffb82fd"
   },
   "outputs": [],
   "source": [
    "# build model layer by layer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Niy6az_HQhc"
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5_NmqZHHQhd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "20752aa3-2c3d-49be-e9b7-8a83c7f031bd"
   },
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model.fit(train_images, \n",
    "          train_labels, \n",
    "          epochs=10, \n",
    "          validation_data=(test_images, test_labels)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8JXvZjDHQhf",
    "outputId": "ac4f4259-078c-443f-941d-9929a957586a"
   },
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will apply CNNs to a classification task in the module project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Digression\n",
    "\n",
    "We do have the option of taking a look at the weight values (from the convolutional layers and FCFF layers). The input data (the images) are normalized to values between 0 and 1. Our lesson on Gradient Descent (from Sprint 2) taught us that normalized data should have 2 observable effects: \n",
    "\n",
    "- The weight values should all be about the same size (i.e. on the same order of magnitude). \n",
    "- The model performance should be better when using normlized data than on non-normalized data set. \n",
    "\n",
    "A fun experiment you should consider runing at some other time would be to see if these observations are reversed and to what extent if the input data is not normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4215734d2fc39cfb8beeff547215810d",
     "grade": false,
     "grade_id": "cell-b23fb65b34cd468a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# model has a bunch of cool attributes that you can explore\n",
    "# one such attribute is the tuned weights between each layer \n",
    "\n",
    "# these are the trained/learned weights of our model when we NORMALIZE our input data\n",
    "# Experiment: what would the weight values be if we DIDN'T NORMALIZE our imput data???????????\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGFiPYCVHQhh"
   },
   "source": [
    "# Transfer Learning for Image Classification (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rCyTJl2HQhh",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic_wzFnprwXI"
   },
   "source": [
    "### Transfer Learning Repositories\n",
    "\n",
    "#### TensorFlow Hub\n",
    "\n",
    "\"A library for reusable machine learning modules\"\n",
    "\n",
    "This lets you quickly take advantage of a model that was trained with thousands of GPU hours. \n",
    "\n",
    "It also enables **transfer learning** - reusing a part of a trained model (called a module) that includes weights and assets, but also training the overall model some yourself with your own data. The advantages are fairly clear - you can use less training data, have faster training, and have a model that generalizes better.\n",
    "\n",
    "https://www.tensorflow.org/hub/\n",
    "\n",
    "TensorFlow Hub is very bleeding edge, and while there's a good amount of documentation out there, it's not always updated or consistent. You'll have to use your problem-solving skills if you want to use it!\n",
    "\n",
    "#### Keras API - Applications\n",
    "\n",
    "> Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n",
    "\n",
    "There is a decent selection of important benchmark models. \n",
    "\n",
    "We'll focus on an image classifier: **ResNet50.**\n",
    "\n",
    "Here's a link that lists all the pre-trained models in the [**Keras Library**](https://keras.io/api/applications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhF4z1aSHQhi"
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM_ApKbGYM9S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM_ApKbGYM9S"
   },
   "outputs": [],
   "source": [
    "def process_img_path(img_path):\n",
    "    \"\"\"\n",
    "    Using tensorflow per-build image processor. \n",
    "\n",
    "    Returns processed image. \n",
    "    \"\"\"\n",
    "    # docs: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img\n",
    "    return image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "def img_contains_banana(img):\n",
    "    \"\"\"\n",
    "    Imputs image into resnet50 pre-trained model and returns the top 3 likely labels for the image (ranked by largest probability)\n",
    "    \"\"\"\n",
    "    # preprocess image\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    # instantiate model\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    \n",
    "    # get classification of image\n",
    "    features = model.predict(x)\n",
    "    \n",
    "    # docs: https://www.tensorflow.org/api_docs/python/tf/keras/applications/imagenet_utils/decode_predictions\n",
    "    results = decode_predictions(features, top=3)[0]\n",
    "    print(results)\n",
    "    \n",
    "    for entry in results:\n",
    "        if entry[1] == 'banana':\n",
    "            return entry[2]\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cQ8ZsJF_Z3B"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# links to two images that we'll use with our pre-trained model\n",
    "image_urls = [\"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/negative_examples/example11.jpeg\",\n",
    "              \"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/positive_examples/example0.jpeg\"]\n",
    "\n",
    "for _id,img in enumerate(image_urls): \n",
    "    r = requests.get(img)\n",
    "    with open(f'example{_id}.jpg', 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "Gxzkai0q_d-4",
    "outputId": "5ed8f74d-b559-4393-838c-b50051eacd87"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./example0.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8NIlClb_n8s",
    "outputId": "ae4c88ef-c094-4b2e-dc8f-42c48a111bf9"
   },
   "outputs": [],
   "source": [
    "processed_imaged = process_img_path('example0.jpg')\n",
    "\n",
    "img_contains_banana(processed_imaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "YIwtRazQ_tQr",
    "outputId": "e7d9e67b-ff66-4635-f1a7-7aad2f400d63"
   },
   "outputs": [],
   "source": [
    "Image(filename='example1.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDXwkPWOAB14",
    "outputId": "0274b6c8-8b05-4d6d-fcb8-ea48e787f342"
   },
   "outputs": [],
   "source": [
    "img_contains_banana(process_img_path('example1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdF5A88oPYvX"
   },
   "source": [
    "Notice that, while it gets it right, the confidence for the banana image is fairly low. That's because so much of the image is \"not-banana\"! How can this be improved? Bounding boxes to center on items of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTAcXO7THQht"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to apply a pretrained model to a classificaiton problem today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCSImm1cHQhu"
   },
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe convolution and pooling\n",
    "    * A Convolution is a function applied to another function to produce a third function\n",
    "    * Convolutional Kernels are typically 'learned' during the process of training a Convolution Neural Network\n",
    "    * Pooling is a dimensionality reduction technique that uses either Max or Average of a feature map region to downsample data\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a convolutional neural network to a classification task\n",
    "    * Keras has layers for convolutions :) \n",
    "- <a href=\"#p3\">Part 3: </a>Transfer Learning for Image Classification\n",
    "    * Check out both pretinaed models available in Keras & TensorFlow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYEX1etrHQhu"
   },
   "source": [
    "# Sources\n",
    "\n",
    "- *_Deep Learning_*. Goodfellow *et al.*\n",
    "- *Hands-on Machine Learnign with Scikit-Learn, Keras & Tensorflow*\n",
    "- [Keras CNN Tutorial](https://www.tensorflow.org/tutorials/images/cnn)\n",
    "- [Tensorflow + Keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n",
    "- [Convolution Wiki](https://en.wikipedia.org/wiki/Convolution)\n",
    "- [Keras Conv2D: Working with CNN 2D Convolutions in Keras](https://missinglink.ai/guides/keras/keras-conv2d-working-cnn-2d-convolutions-keras/)\n",
    "- [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "- [A Beginner's Guide to Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHhtQf8Fr27W"
   },
   "source": [
    "------\n",
    "# Referenced used duirng lecutre \n",
    "\n",
    "[Image of Tensors](https://miro.medium.com/max/891/0*jGB1CGQ9HdeUwlgB), we learned that vectors and Matricies are special cases of a more general data structure called a Tensor. \n",
    "\n",
    "[CNN Feature Engineering](https://qjjnh3a9hpo1nukrg1fwoh71-wpengine.netdna-ssl.com/wp-content/uploads/2019/07/1_ZD3ewOfpfsMAjhp4MYFnog-edited.jpg), we learned that each convolutional layer is creating features from the images that are passed into it. This image shows how a CNN learns edges, object parts, and then the entire object as learned features from previous convolutional layers are passed to sunsequent convolutional layers. \n",
    "\n",
    "[Stanford University CNN class](https://cs231n.github.io/convolutional-networks/), in lecture I refered the annimation at about half way down the page that shows how the filter matrices (i.e. the weight matrices) are overlapped over the Red, Yellow, and Blue channels of a color image in order to calculate convolutions and how the results are stored in an output volume. \n",
    "\n",
    "[Py Image Search](https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/#:~:text=increase%20when%20necessary.-,kernel_size,(7%2C%207)%20tuples.&text=%2C%20a%202%2Dtuple%20specifying%20the,of%20the%202D%20convolution%20window.), this link provdies an in-depth exploration of each of the Conv2D parameters and how to select specific values for each parameter. \n",
    "\n",
    "[Keras Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) and of course don't forget about the documention for the python package that we are using to build our models!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LS_DS_432_Convolutional_Neural_Networks_Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37  (Python3)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
